{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why neural networks?\n",
    "\n",
    "Notebook by José Carlos Martínez Velázquez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a neuron works? (McCulloch-Pitts, 1943)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./NeuralNetworks_img/neuron.png\" width=\"30%\">\n",
    "\n",
    "We have $n$ inputs (numbers): $\\vec{x}^T = (x_1,...,x_n)$, $n$ weights (numbers): $\\vec{x}^T = (w_1,...,w_n)$, a bias also called threshold (number): $\\theta$ and also we have an activation function $\\phi$. Originally the activation function is this:\n",
    "\n",
    "$$\n",
    " \\phi(x)= \\left\\{ \\begin{array}{lcc}\n",
    "             1 &   if  & x \\geq 0 \\\\\n",
    "             0 &   if  & x < 0 \\\\\n",
    "             \\end{array}\n",
    "   \\right.\n",
    "$$\n",
    "\n",
    "This activation function aims to imitate the biological excitation or inhibition of a neuron.\n",
    "\n",
    "At the end of the day, the output ($\\hat{y}$) of a neuron is computed like this:\n",
    "\n",
    "$$\\hat{y} = \\phi(x_1w_1 + ... + x_nw_n + \\theta) = \\sum_{i=1}^n x_i w_i + \\theta$$\n",
    "\n",
    "Just for be compact, we add in $\\vec{w}$ a new component $w_0 = \\theta$ and $x_0 = 1$ in $\\vec{x}$. If we do so, we can calculate the output of a neuron as:\n",
    "\n",
    "$$\\hat{y} = \\phi(w^Tx) $$\n",
    "\n",
    "The $\\vec{w}$ values are the parameters that the model must learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems with the neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity in the argument, we fix $n=2$. In this case, we have the following components:\n",
    "\n",
    "* $\\vec{x}^T = (1,x_1,x_2)$\n",
    "* $\\vec{w}^T = (w_0,w_1,w_2)$\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\\hat{y} = \\phi(w_0 + x_1w_1 + x_2w_2)$$\n",
    "\n",
    "If you look carefully at the argument of $\\phi$ what the model is learning is the parameters of a straight line.\n",
    "\n",
    "Well... Let's check at the XOR problem... ¿Could you draw a straight line separating the \"+\" and \"-\" categories?\n",
    "\n",
    "<img src=\"./NeuralNetworks_img/xor_problem_unsolved.png\" width=\"30%\">\n",
    "\n",
    "Solution: The first neural network, defining two different regions\n",
    "\n",
    "<img src=\"./NeuralNetworks_img/xor_problem.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parts of a basic (fully connected) neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the first artificial neural network (ANN), new concepts were defined and modifications were performed. Let's introduce some notation. Imagine a neural network with 4 inputs, two hidden layers (two and three neurons respectively) and one output layer with only a neuron (output layers can have more than one neuron).\n",
    "\n",
    "\n",
    "<img src=\"./NeuralNetworks_img/nn_example.png\" width=\"40%\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "In order to have a simple neural network, we need to know the following concepts:\n",
    "\n",
    "* Input Layer vs. Hidden layers vs. Output layers: we need to disgregate between input, hidden and output layers. Input layer contains just the $\\vec{x}$ values, hidden layers contains the values of activations in the previous layers and output layer contains the prediction for the input.\n",
    "\n",
    "* Activation function: computes the intermediate activation value for a whole layer, feeding the next one.\n",
    "* Forward propagation: calculate the output computing the activation of all neurons in all layers. Let's see an example in the previous network:\n",
    "\n",
    "    the activation of the first hidden layer is calculated by\n",
    "\n",
    "    $$A^{[1]} = \\phi \\left(\n",
    "    \\begin{bmatrix}\n",
    "    1 & x_1 & x_2 & x_3 & x_4\\\\ \n",
    "    1 & x_1 & x_2 & x_3 & x_4\n",
    "    \\end{bmatrix}\n",
    "    \\cdot \\begin{bmatrix}\n",
    "    w^{[1]}_{00} & w^{[1]}_{10}\\\\ \n",
    "    w^{[1]}_{01} & w^{[1]}_{11}\\\\ \n",
    "    w^{[1]}_{02} & w^{[1]}_{12}\\\\ \n",
    "    w^{[1]}_{03} & w^{[1]}_{13}\\\\\n",
    "    w^{[1]}_{04} & w^{[1]}_{14}\n",
    "    \\end{bmatrix}\\right) = \\phi \\left(\n",
    "    \\vec{x}\n",
    "    \\cdot W^{[1]}\\right)$$\n",
    "\n",
    "    Where $w^{[l]}_{n,i}$ means the component $i$ of the weight vector of the neuron $n$ in the layer $l$.\n",
    "\n",
    "    Chaining the process we have that:\n",
    "    \n",
    "    $$A^{[l]} = \\phi \\left( [1\\ |\\ A^{[l-1]}] \\cdot W^{[l]}\\right)$$\n",
    "\n",
    "* Loss function: from a supervised learning point of view, we can define a loss function that measures how far are reality from prediction. Imagine that your example $\\vec{x}$ is labeled by $y$. We can define a function $L(y,\\hat{y})$ that computes how far $\\hat{y}$ is from $y$.\n",
    "\n",
    "* Backpropagation: Is the \"magic\" of the neural networks. When we compute forward propagation for an example, we can re-calculate weights in order to minimize the loss function. Backpropagation computes gradients (derivatives) of the loss function with respect to each weight and reasign the weights by any learning rule. The most famous is the gradient descent learning rule:\n",
    "    \n",
    "    $$w_{new} = w_{previous} - \\eta \\frac{\\partial L}{w_{previous}}$$\n",
    "    \n",
    "    where $\\eta$ is a number (usually less than 1) called learning rate, that controls how big are steps in the learning algorithm. The adjustment of $\\eta$ is not easy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking a little example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "from skimage.transform import resize\n",
    "import pickle\n",
    "from scipy.io import loadmat\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resize the data to 100x100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'cars_train\\\\'\n",
    "print(\"Reading training set...\")\n",
    "image_names = os.listdir(train_path)\n",
    "num_images = len(image_names)\n",
    "\n",
    "train = []\n",
    "for i,name in enumerate(image_names):\n",
    "    train.append(resize(plt.imread(train_path+name), output_shape = (100,100,3), mode='reflect'))\n",
    "    if i%100 == 0:\n",
    "        print(f'processed image {i}/{num_images}')\n",
    "\n",
    "train = np.array(train)\n",
    "\n",
    "with open('pickled_train','wb') as fp:\n",
    "    pickle.dump(train,fp)\n",
    "\n",
    "#train = np.array([resize(plt.imread(train_path+name), output_shape = (100,100,3), mode='reflect') for name in os.listdir(train_path)])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading, adjusting and saving training labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('car_devkit/train_perfect_preds.txt','r') as f:\n",
    "    train_labels = f.readlines()\n",
    "\n",
    "train_labels = [int(x.strip())-1 for x in train_labels] \n",
    "\n",
    "with open('pickled_train_labels','wb') as fp:\n",
    "    pickle.dump(train_labels,fp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see a sample of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickled_train','rb') as fp:\n",
    "    train=pickle.load(fp)\n",
    "    \n",
    "with open('pickled_train_labels','rb') as fp:\n",
    "    train_labels=pickle.load(fp)\n",
    "    \n",
    "\n",
    "labels_info=loadmat('car_devkit/cars_meta.mat')\n",
    "labels_mapping = {i:c[0] for i,c in enumerate(labels_info['class_names'][0])}\n",
    "\n",
    "train_len = train.shape[0]\n",
    "nfigs=3\n",
    "\n",
    "idx_to_show = np.random.choice(np.arange(train_len), size=nfigs*nfigs, replace=False)\n",
    "\n",
    "fig,ax = plt.subplots(nfigs,nfigs,figsize=(15,15))\n",
    "\n",
    "for i in range(nfigs):\n",
    "    for j in range(nfigs):\n",
    "        ax[i,j].set_title(labels_mapping[train_labels[idx_to_show[ i*nfigs+j ]]])\n",
    "        ax[i,j].imshow(train[idx_to_show[ i*nfigs+j ]])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Is data balanced?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(train_labels)\n",
    "mapped_labels=[labels_mapping[l] for l in train_labels]\n",
    "proportion = pd.value_counts(mapped_labels)/len(mapped_labels)\n",
    "print(f'Minimum proportion: {min(proportion)}')\n",
    "print(f'Minimum proportion: {max(proportion)}')\n",
    "print(f'Highest difference: {max(proportion)-min(proportion)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data in order to make it able to enter into network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_train = np.array([ e.flatten() for e in train ])\n",
    "print(f'Real train shape: {train.shape}')\n",
    "print(f'Flatten train shape: {flatten_train.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_flatten_train, shuffled_train_labels = shuffle(flatten_train,train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_show = np.random.choice(np.arange(train_len), size=nfigs*nfigs, replace=False)\n",
    "\n",
    "fig,ax = plt.subplots(nfigs,nfigs,figsize=(15,15))\n",
    "\n",
    "for i in range(nfigs):\n",
    "    for j in range(nfigs):\n",
    "        ax[i,j].set_title(labels_mapping[train_labels[idx_to_show[ i*nfigs+j ]]])\n",
    "        ax[i,j].imshow(train[idx_to_show[ i*nfigs+j ]])\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_shuffled_train_labels = to_categorical(shuffled_train_labels,np.unique(shuffled_train_labels).shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "ntrheads = '32'\n",
    "os.environ['MKL_NUM_THREADS'] = ntrheads\n",
    "os.environ['GOTO_NUM_THREADS'] = ntrheads\n",
    "os.environ['OMP_NUM_THREADS'] = ntrheads\n",
    "os.environ['openmp'] = 'True'\n",
    "\n",
    "model = Sequential()\n",
    "model.add( Dense(256, input_shape = (flatten_train.shape[1],), activation = 'relu',kernel_initializer='glorot_normal', name = 'my_hidden_layer') )\n",
    "model.add( Dense(128, activation = 'relu',kernel_initializer='glorot_normal', name = 'my_hidden_layer1') )\n",
    "model.add( Dense(len(labels_mapping.keys()), activation = 'softmax',kernel_initializer='glorot_normal' , name = 'my_output_layer') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-04, decay=0.0, amsgrad=True)\n",
    "model.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.fit(shuffled_flatten_train, one_hot_shuffled_train_labels, epochs=10, batch_size=8, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
